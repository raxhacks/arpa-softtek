{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pinecone import ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "from openai import OpenAI\n",
    "import pinecone \n",
    "import time\n",
    "import os\n",
    "from google.auth import compute_engine\n",
    "from google.cloud import firestore\n",
    "from langchain_google_firestore import FirestoreChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = firestore.Client(\n",
    "    project=\"arpa-softtek\",\n",
    "    database=\"(default)\",\n",
    "    credentials=compute_engine.Credentials(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "MODEL = \"text-embedding-3-small\"\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_texto(url):\n",
    "    loader = PyPDFLoader(\"/Users/luisbarajas/Documents/AGI/Papers/weak-to-strong-generalization.pdf\")\n",
    "    data = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(data)\n",
    "    return texts\n",
    "\n",
    "docs = obtener_texto('./model2.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'arpa2'\n",
    "spec = ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# check if index already exists (it shouldn't if this is your first run)\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name=index_name,\n",
    "        dimension=1536,  # dimensionality of text-embed-3-small\n",
    "        metric='cosine',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "        \n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=OpenAIEmbeddings(model=MODEL)) #cuando ya existe el index\n",
    "\n",
    "# cuando tdv no existe el index\n",
    "# vectorstore_from_documents = PineconeVectorStore.from_documents(\n",
    "#         docs,\n",
    "#         index_name=index_name,\n",
    "#         embedding=OpenAIEmbeddings(model=MODEL),\n",
    "# )\n",
    "\n",
    "retreiver = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "  context: RunnableLambda(format_docs)\n",
      "}), config={'run_name': 'format_inputs'})\n",
      "| ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an assistant for question-answering tasks related to scientific papers, articles and investigations. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n{context}\")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
      "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x12456d610>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x11780dcd0>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
      "| StrOutputParser() config={'run_name': 'stuff_documents_chain'}\n",
      "input_variables=['chat_history', 'context', 'input'] input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an assistant for question-answering tasks related to scientific papers, articles and investigations. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n{context}\")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))]\n"
     ]
    }
   ],
   "source": [
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retreiver, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "### Answer question ###\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks related to scientific papers, articles and investigations. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "print(question_answer_chain)\n",
    "print(qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Statefully manage chat history ###\n",
    "\n",
    "#importar credenciales formato main de functions \n",
    "# llamar las funciones de firebase de libreria client\n",
    "# (crear collection extra para pruebas)\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store: #Funcion para ver si existe un historial de chat    \n",
    "        # Create a new FirestoreChatMessageHistory instance\n",
    "        chat_history = FirestoreChatMessageHistory(session_id=session_id, collection=\"chat\")\n",
    "    else:\n",
    "        chat_history = FirestoreChatMessageHistory(session_id=session_id, collection=\"chat\")\n",
    "    return chat_history\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing keys ['session_id'] in config['configurable'] Expected keys are ['session_id'].When using via .invoke() or .stream(), pass in a config; e.g., chain.invoke({'input': 'foo'}, {'configurable': {'session_id': '[your-value-here]'}})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconversational_rag_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat is the paper about?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# constructs a key \"abc123\" in `store`.\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/runnables/base.py:4519\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4512\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4513\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4514\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4515\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4516\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m   4517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m   4518\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m-> 4519\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   4520\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   4521\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/runnables/history.py:463\u001b[0m, in \u001b[0;36mRunnableWithMessageHistory._merge_configs\u001b[0;34m(self, *configs)\u001b[0m\n\u001b[1;32m    459\u001b[0m     example_configurable \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    460\u001b[0m         missing_key: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[your-value-here]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m missing_key \u001b[38;5;129;01min\u001b[39;00m missing_keys\n\u001b[1;32m    461\u001b[0m     }\n\u001b[1;32m    462\u001b[0m     example_config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: example_configurable}\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing keys \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(missing_keys)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in config[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected keys are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(expected_keys)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen using via .invoke() or .stream(), pass in a config; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me.g., chain.invoke(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     )\n\u001b[1;32m    470\u001b[0m parameter_names \u001b[38;5;241m=\u001b[39m _get_parameter_names(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_session_history)\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(expected_keys) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# If arity = 1, then invoke function by positional arguments\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Missing keys ['session_id'] in config['configurable'] Expected keys are ['session_id'].When using via .invoke() or .stream(), pass in a config; e.g., chain.invoke({'input': 'foo'}, {'configurable': {'session_id': '[your-value-here]'}})"
     ]
    }
   ],
   "source": [
    "prompt = \"what is the paper about?\"\n",
    "answer = conversational_rag_chain.invoke(\n",
    "    {\"input\": prompt},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc124\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]\n",
    "\n",
    "chat_history.add_user_message(prompt)\n",
    "chat_history.add_ai_message(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
