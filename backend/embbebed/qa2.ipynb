{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from openai import OpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "PINECONE_API_ENV = os.getenv('PINECONE_API_ENV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)   \n",
    "index_name = \"arpa\"\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "# vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_chroma.vectorstores.Chroma object at 0x169cf9c50>\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"/Users/luisbarajas/Documents/AGI/Papers/weak-to-strong-generalization.pdf\")\n",
    "data = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(data)\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "# print(docs)\n",
    "# res = client.embeddings.create(input=docs, model=MODEL)\n",
    "# print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Chroma' object has no attribute 'get_vector'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweak\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Chroma' object has no attribute 'get_vector'"
     ]
    }
   ],
   "source": [
    "print(vectorstore.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "### Answer question ###\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks related to scientific papers, articles and investigations. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Statefully manage chat history ###\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The last paper mentioned in the context is \"Multiaccuracy: Black-box post-processing for fairness in classification\" by Michael P Kim, Amirata Ghorbani, and James Zou. This paper discusses the concept of multiaccuracy, which is a black-box post-processing technique used to enhance fairness in classification tasks. The focus is on addressing fairness concerns in machine learning models by adjusting the decision boundaries to achieve fairness across different demographic groups.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"what was the last paper about?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"luis\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]\n",
    "# print(store)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
